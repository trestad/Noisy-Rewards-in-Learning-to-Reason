def _validate(self):
        reward_tensor_lst = []
        task_lst = []
        for test_data in self.val_dataloader:
            test_batch = DataProto.from_single_dict(test_data)
            # test_batch = test_batch.to('cuda')

            # we only do validation on rule-based rm
            if self.config.reward_model.enable and test_batch[0].non_tensor_batch['reward_model']['style'] == 'model':
                return {}

            test_gen_batch = test_batch.pop(['input_ids', 'attention_mask', 'position_ids'])
            test_gen_batch.meta_info = {
                'eos_token_id': self.tokenizer.eos_token_id,
                'pad_token_id': self.tokenizer.pad_token_id,
                'recompute_log_prob': False,
                'do_sample': False,
                'validate': True,
            }

            # pad to be divisible by dp_size
            test_gen_batch_padded, pad_size = pad_dataproto_to_divisor(test_gen_batch, self.actor_rollout_wg.world_size)
            print(f'\n\nvalidation {len(test_gen_batch_padded)} data.\n\n')
            test_output_gen_batch_padded = self.actor_rollout_wg.generate_sequences(test_gen_batch_padded)
            # unpad
            test_output_gen_batch = unpad_dataproto(test_output_gen_batch_padded, pad_size=pad_size)
            print('validation generation end')

            test_batch = test_batch.union(test_output_gen_batch)

            # evaluate using reward_function
            # for certain reward function (e.g. sandbox), the generation can overlap with reward
            reward_tensor = self.val_reward_fn(test_batch)

            reward_tensor_lst.append(reward_tensor)
            task_lst.append(test_batch.non_tensor_batch.get('task', ['unknown'] * reward_tensor.shape[0]))

        reward_tensor = torch.cat(reward_tensor_lst, dim=0).sum(-1).cpu()  # (batch_size,)
        tasks = np.concatenate(task_lst, axis=0)
        # evaluate test_score based on data source
        task_reward = {}
        for i in range(reward_tensor.shape[0]):
            task = tasks[i]
            if task not in task_reward:
                task_reward[task] = []
            task_reward[task].append(reward_tensor[i].item())

        metric_dict = {}
        for task, rewards in task_reward.items():
            count_equal_3 = sum(1 for reward in rewards if reward == 3.5)
            total_count = len(rewards)
            metric_dict[f'val/test_score/{task}'] = count_equal_3 / total_count if total_count > 0 else 0

        return metric_dict